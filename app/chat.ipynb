{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba42292",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "This notebook implements a modular Retrieval-Augmented Generation (RAG) pipeline for question answering over PDF documents. The system extracts text from PDFs, embeds and retrieves relevant chunks, and generates fluent answers using transformer-based models. Each step is documented below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76df654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c031b60d",
   "metadata": {},
   "source": [
    "## Step 1: Extract Text from PDF\n",
    "\n",
    "We use PyMuPDF to extract layout-aware text from the uploaded PDF. This preserves reading order and structure, which is important for downstream retrieval. The extracted text is stored as a single string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb588df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The History of Paper \n",
      " \n",
      "Paper has been one of the most transformative inventions in human history. Originating in \n",
      "China around 105 AD, it replaced earlier writing surfaces like papyrus and parchment. Cai \n",
      "Lun, a Chinese court official, is credited with refining the papermaking process using \n",
      "mulberry bark, hemp, and rags. \n",
      " \n",
      "Over the centuries, papermaking spread across Asia, the Middle East, and Europe. By the \n",
      "13th century, paper mills were operating in Spain and Italy, revolutionizing communication \n",
      "and record-keeping. \n",
      " \n",
      "In the modern era, paper is made primarily from wood pulp, and its uses range from books \n",
      "and newspapers to packaging and hygiene products. Despite the rise of digital media, \n",
      "paper remains a vital part of global infrastructure. \n",
      " \n",
      "Fun Fact: \n",
      "The word “paper” comes from “papyrus,” a plant-based writing material used in ancient \n",
      "Egypt. \n",
      " \n",
      "Page 2 \n",
      " \n",
      "Environmental Impact \n",
      " \n",
      "While paper is recyclable and biodegradable, its production can be resource-intensive. \n",
      "Sustai\n"
     ]
    }
   ],
   "source": [
    "# pdf extraction\n",
    "def extracttextpymupdf(path):\n",
    "    doc = fitz.open(path)\n",
    "    return \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "def extracttablespdfplumber(path):\n",
    "    tables = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            tables.extend(page.extract_tables())\n",
    "    return tables\n",
    "\n",
    "pdf_path = \"../data/pdfchat_sample.pdf\"  \n",
    "raw_text = extracttextpymupdf(pdf_path)\n",
    "print(raw_text[:1000])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bce276",
   "metadata": {},
   "source": [
    "## Step 2: Chunk the Extracted Text\n",
    "\n",
    "To enable efficient retrieval, we split the raw text into fixed-size chunks. Each chunk becomes a unit of semantic representation and retrieval. We use a simple word-based chunking strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ad3066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1\n"
     ]
    }
   ],
   "source": [
    "#chunking\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "chunks = chunk_text(raw_text)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452eea2a",
   "metadata": {},
   "source": [
    "## Step 3: Embed Chunks Using BERT\n",
    "\n",
    "Each chunk is embedded into a dense vector using BERT (bert-base-uncased). These embeddings capture semantic meaning and are used for similarity-based retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90862690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding with BERT\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def embed(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "chunk_embeddings = [embed(c, bert_tokenizer, bert_model) for c in chunks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa9650",
   "metadata": {},
   "source": [
    "## Step 4: Embed Question and Retrieve Relevant Chunks\n",
    "\n",
    "The user’s question is embedded using DistilBERT. Cosine similarity is computed between the question and each chunk embedding. The top-k most relevant chunks are selected as context for answer generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d03b2562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The History of Paper Paper has been one of the most transformative inventions in human history. Originating in China around 105 AD, it replaced earlier writing surfaces like papyrus and parchment. Cai Lun, a Chinese court official, is credited with refining the papermaking process using mulberry bark, hemp, and rags. Over the centuries, papermaking spread across Asia, the Middle East, and Europe. By the 13th century, paper mills were operating in Spain and Italy, revolutionizing communication and record-keeping. In the modern era, paper is made primarily from wood pulp, and its uses range from books and newspapers to packaging and hygiene products. Despite the rise of digital media, paper remains a vital part of global infrastructure. Fun Fact: The word “paper” comes from “papyrus,” a plant-based writing material used in ancient Egypt. Page 2 Environmental Impact While paper is recyclable and biodegradable, its production can be resource-intensive. Sustainable forestry practices and recycled paper initiatives aim to reduce the ecological footprint of the industry. Digital alternatives have reduced paper consumption in some sectors, but demand remains strong in education, packaging, and printing. Page 3 Conclusion Paper’s journey from ancient China to modern printing presses reflects humanity’s evolving need to record, share, and preserve information. Whether in books, ballots, or blueprints, paper continues to shape our world.\n"
     ]
    }
   ],
   "source": [
    "#Question Embedding + Retrieval\n",
    "retrieval_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "retrieval_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def embedquestion(question):\n",
    "    inputs = retrieval_tokenizer(question, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = retrieval_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "def retrievetopchunks(question, chunk_embeddings, chunks, k=3):\n",
    "    q_embed = embedquestion(question)\n",
    "    scores = cosine_similarity([q_embed], chunk_embeddings)[0]\n",
    "    top_indices = np.argsort(scores)[-k:][::-1]\n",
    "    return [chunks[i] for i in top_indices]\n",
    "\n",
    "question = \"Where was paper first invented?\"\n",
    "top_chunks = retrievetopchunks(question, chunk_embeddings, chunks)\n",
    "print(\"\\n\\n\".join(top_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81ee01",
   "metadata": {},
   "source": [
    "## Step 5: Generate Answer Using FLAN-T5\n",
    "\n",
    "The selected chunks are concatenated and passed to FLAN-T5 (flan-t5-base), a fine-tuned instruction-following model. The model generates a fluent answer conditioned on the retrieved context and the user’s question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b02e6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Answer: China\n"
     ]
    }
   ],
   "source": [
    "#Answer Generation with FLAN-T5\n",
    "flan_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "flan_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "def generate_answer(question, context_chunks):\n",
    "    context = \"\\n\".join(context_chunks)\n",
    "    prompt = f\"Answer the question based on this context:\\n{context}\\n\\nQuestion: {question}\"\n",
    "    inputs = flan_tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = flan_model.generate(**inputs, max_length=200)\n",
    "    return flan_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "answer = generate_answer(question, top_chunks)\n",
    "print(f\"\\n Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cbe5f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The History of Paper \n",
      " \n",
      "Paper has been one of the most transformative inventions in human history. Originating in \n",
      "China around 105 AD, it replaced earlier writing surfaces like papyrus and parchment. Cai \n",
      "Lun, a Chinese court official, is credited with refining the papermaking process using \n",
      "mulberry bark, hemp, and rags. \n",
      " \n",
      "Over the centuries, papermaking spread across Asia, the Middle East, and Europe. By the \n",
      "13th century, paper mills were operating in Spain and Italy, revolutionizing communication \n",
      "and record-keeping. \n",
      " \n",
      "In the modern era, paper is made primarily from wood pulp, and its uses range from books \n",
      "and newspapers to packaging and hygiene products. Despite the rise of digital media, \n",
      "paper remains a vital part of global infrastructure. \n",
      " \n",
      "Fun Fact: \n",
      "The word “paper” comes from “papyrus,” a plant-based writing material used in ancient \n",
      "Egypt. \n",
      " \n",
      "Page 2 \n",
      " \n",
      "Environmental Impact \n",
      " \n",
      "While paper is recyclable and biodegradable, its production can be resource-intensive. \n",
      "Sustai\n"
     ]
    }
   ],
   "source": [
    "# PDF extraction using PyMuPDF\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_pymupdf(path):\n",
    "    doc = fitz.open(path)\n",
    "    return \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "# Adjust path based on notebook location\n",
    "pdf_path = \"../data/pdfchat_sample.pdf\"  \n",
    "raw_text = extract_text_pymupdf(pdf_path)\n",
    "\n",
    "print(raw_text[:1000])  # Preview first 1000 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64121113",
   "metadata": {},
   "source": [
    "## Step 6 : Evaluate Generated Answer\n",
    "\n",
    "To assess the quality of generated answers, we use ROUGE and BLEU metrics. These compare the generated output against a reference answer, measuring lexical overlap and n-gram precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec47f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE: {'rouge1': np.float64(0.2857142857142857), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.2857142857142857), 'rougeLsum': np.float64(0.2857142857142857)}\n",
      "BLEU: {'bleu': 0.0, 'precisions': [1.0, 0.0, 0.0, 0.0], 'brevity_penalty': 0.0024787521766663585, 'length_ratio': 0.14285714285714285, 'translation_length': 1, 'reference_length': 7}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "reference = \"Paper was first invented in China.\"\n",
    "prediction = answer  # From FLAN-T5\n",
    "\n",
    "# ROUGE: accepts strings\n",
    "rouge_scores = rouge.compute(predictions=[prediction], references=[reference])\n",
    "\n",
    "# BLEU: accepts strings or list of strings\n",
    "bleu_scores = bleu.compute(predictions=[prediction], references=[[reference]])\n",
    "\n",
    "print(\"ROUGE:\", rouge_scores)\n",
    "print(\"BLEU:\", bleu_scores)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
